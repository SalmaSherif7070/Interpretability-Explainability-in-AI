# Interpretability & Explainability in AI

A Review Paper Repository with 12 Paper Implementations, Model Explanations, and Interpretations

## ðŸ“Œ Overview

This repository accompanies a comprehensive review paper on interpretability and explainability techniques in AI, featuring:

- 12 key papers implemented with code, datasets, and results.
- Model explanations using tools like LIME, SHAP, Grad-CAM, and DeepLIFT.
- Critical analysis of each methodâ€™s strengths, limitations.
- You can find the data in kaggle at: https://www.kaggle.com/datasets/salmasherif202200622/feature-selelcted-dataset
  

ðŸ”— Roadmap: For a structured guide to navigating this repo (e.g., paper list, models, results, explanation, etc), click [HERE](https://docs.google.com/document/d/1BYylULjdUU93eQL_jrLfBk9318yNvyy9QFbydm9vz3o/edit?usp=sharing)

## ðŸŽ¯ Motivation

As AI models grow more complex, understanding their decisions becomes crucial, especially in predicting COVID-19 from X-ray images. This project:
1. Compares interpretability methods.
2. Provides reproducible implementations for practitioners.

## ðŸš€ Quick Start
1. Clone the repo:
```bash
git clone https://github.com/SalmaSherif7070/Interpretability-Explainability-in-AI.git
```
2. Follow the our [Report](https://docs.google.com/document/d/1BYylULjdUU93eQL_jrLfBk9318yNvyy9QFbydm9vz3o/edit?usp=sharing) to explore specific papers or datasets.
3. Run Jupyter notebooks in papers/ to reproduce results.

## Contributing

Pull requests are welcome. For major changes, please open an issue first
to discuss what you would like to change.

Please make sure to update tests as appropriate.

## License

[MIT](https://choosealicense.com/licenses/mit/)
